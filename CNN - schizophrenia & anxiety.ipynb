{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d81ef43",
   "metadata": {
    "id": "4d81ef43"
   },
   "source": [
    "# MindInsight Classifier: Unveiling Mental Health Patterns in Pandemic Discourse through Data-Driven Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fff27a",
   "metadata": {
    "id": "d0fff27a"
   },
   "source": [
    "Let us first import the pertinent libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75f7a3d1",
   "metadata": {
    "id": "75f7a3d1"
   },
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5a0274",
   "metadata": {
    "id": "ec5a0274"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f385544",
   "metadata": {
    "id": "6f385544"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('mental_disorders_reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d476b8d",
   "metadata": {
    "id": "1d476b8d",
    "outputId": "55a3090c-928f-4646-c170-11838f1bfee8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Life is so pointless without others</td>\n",
       "      <td>Does anyone else think the most important part...</td>\n",
       "      <td>1650356960</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cold rage?</td>\n",
       "      <td>Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...</td>\n",
       "      <td>1650356660</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I donâ€™t know who I am</td>\n",
       "      <td>My [F20] bf [M20] told me today (after I said ...</td>\n",
       "      <td>1650355379</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HELP! Opinions! Advice!</td>\n",
       "      <td>Okay, Iâ€™m about to open up about many things I...</td>\n",
       "      <td>1650353430</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1650350907</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  Life is so pointless without others   \n",
       "1                           Cold rage?   \n",
       "2                I donâ€™t know who I am   \n",
       "3              HELP! Opinions! Advice!   \n",
       "4                                 help   \n",
       "\n",
       "                                            selftext  created_utc  over_18  \\\n",
       "0  Does anyone else think the most important part...   1650356960    False   \n",
       "1  Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...   1650356660    False   \n",
       "2  My [F20] bf [M20] told me today (after I said ...   1650355379    False   \n",
       "3  Okay, Iâ€™m about to open up about many things I...   1650353430    False   \n",
       "4                                          [removed]   1650350907    False   \n",
       "\n",
       "  subreddit  \n",
       "0       BPD  \n",
       "1       BPD  \n",
       "2       BPD  \n",
       "3       BPD  \n",
       "4       BPD  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea1238",
   "metadata": {
    "id": "89ea1238"
   },
   "source": [
    "### Data Preprocessing and Simple EDA (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf6c318",
   "metadata": {
    "id": "fbf6c318",
    "outputId": "78c2a53d-3563-47ef-ab21-5de93144781f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(701787, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0db22d",
   "metadata": {
    "id": "af0db22d",
    "outputId": "8ac4d457-cdbd-47f7-b9cd-245a04e1cb17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             46\n",
       "selftext       33691\n",
       "created_utc        0\n",
       "over_18            0\n",
       "subreddit          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc7f05c",
   "metadata": {
    "id": "ffc7f05c"
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['selftext'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5786258d",
   "metadata": {
    "id": "5786258d",
    "outputId": "499b4e3e-3204-4c86-9f2a-611696d7b6cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          42\n",
       "selftext        0\n",
       "created_utc     0\n",
       "over_18         0\n",
       "subreddit       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae98b60",
   "metadata": {
    "id": "2ae98b60",
    "outputId": "2a409d0f-962b-4349-f4e3-ae08afe503b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BPD              233125\n",
       "Anxiety          167059\n",
       "depression       156717\n",
       "bipolar           46666\n",
       "mentalillness     44249\n",
       "schizophrenia     20280\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f63d3a2",
   "metadata": {
    "id": "8f63d3a2"
   },
   "outputs": [],
   "source": [
    "df['title'] = df['title'].fillna('')\n",
    "\n",
    "# Calculate the total number of words in 'title'\n",
    "df['title_total'] = df['title'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Define a function to count total characters in a text (excluding spaces)\n",
    "def count_total_words(text):\n",
    "    char = 0\n",
    "    for word in text.split():\n",
    "        char += len(word)\n",
    "    return char\n",
    "\n",
    "# Calculate the total number of characters in 'title'\n",
    "df['title_chars'] = df['title'].apply(count_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e56c20",
   "metadata": {
    "id": "18e56c20",
    "outputId": "9e6459e4-73f8-4e02-aaa4-323842facb53"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_total</th>\n",
       "      <th>title_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Life is so pointless without others</td>\n",
       "      <td>Does anyone else think the most important part...</td>\n",
       "      <td>1650356960</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cold rage?</td>\n",
       "      <td>Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...</td>\n",
       "      <td>1650356660</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I donâ€™t know who I am</td>\n",
       "      <td>My [F20] bf [M20] told me today (after I said ...</td>\n",
       "      <td>1650355379</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HELP! Opinions! Advice!</td>\n",
       "      <td>Okay, Iâ€™m about to open up about many things I...</td>\n",
       "      <td>1650353430</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1650350907</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  Life is so pointless without others   \n",
       "1                           Cold rage?   \n",
       "2                I donâ€™t know who I am   \n",
       "3              HELP! Opinions! Advice!   \n",
       "4                                 help   \n",
       "\n",
       "                                            selftext  created_utc  over_18  \\\n",
       "0  Does anyone else think the most important part...   1650356960    False   \n",
       "1  Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...   1650356660    False   \n",
       "2  My [F20] bf [M20] told me today (after I said ...   1650355379    False   \n",
       "3  Okay, Iâ€™m about to open up about many things I...   1650353430    False   \n",
       "4                                          [removed]   1650350907    False   \n",
       "\n",
       "  subreddit  title_total  title_chars  \n",
       "0       BPD            6           30  \n",
       "1       BPD            2            9  \n",
       "2       BPD            6           16  \n",
       "3       BPD            3           21  \n",
       "4       BPD            1            4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c477c22",
   "metadata": {
    "id": "9c477c22"
   },
   "outputs": [],
   "source": [
    "df['text_total'] = df['selftext'].apply(lambda x: len(x.split()))\n",
    "\n",
    "def count_total_words(text):\n",
    "    char = 0\n",
    "    for word in text.split():\n",
    "        char += len(word)\n",
    "    return char\n",
    "\n",
    "df['text_chars'] = df[\"selftext\"].apply(count_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "472c5307",
   "metadata": {
    "id": "472c5307",
    "outputId": "a4ca2b65-baea-4b4e-b21f-9375f1b48245"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_total</th>\n",
       "      <th>title_chars</th>\n",
       "      <th>text_total</th>\n",
       "      <th>text_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Life is so pointless without others</td>\n",
       "      <td>Does anyone else think the most important part...</td>\n",
       "      <td>1650356960</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>74</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cold rage?</td>\n",
       "      <td>Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...</td>\n",
       "      <td>1650356660</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>517</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I donâ€™t know who I am</td>\n",
       "      <td>My [F20] bf [M20] told me today (after I said ...</td>\n",
       "      <td>1650355379</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>145</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HELP! Opinions! Advice!</td>\n",
       "      <td>Okay, Iâ€™m about to open up about many things I...</td>\n",
       "      <td>1650353430</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>821</td>\n",
       "      <td>3282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1650350907</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  Life is so pointless without others   \n",
       "1                           Cold rage?   \n",
       "2                I donâ€™t know who I am   \n",
       "3              HELP! Opinions! Advice!   \n",
       "4                                 help   \n",
       "\n",
       "                                            selftext  created_utc  over_18  \\\n",
       "0  Does anyone else think the most important part...   1650356960    False   \n",
       "1  Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...   1650356660    False   \n",
       "2  My [F20] bf [M20] told me today (after I said ...   1650355379    False   \n",
       "3  Okay, Iâ€™m about to open up about many things I...   1650353430    False   \n",
       "4                                          [removed]   1650350907    False   \n",
       "\n",
       "  subreddit  title_total  title_chars  text_total  text_chars  \n",
       "0       BPD            6           30          74         310  \n",
       "1       BPD            2            9         517        2259  \n",
       "2       BPD            6           16         145         545  \n",
       "3       BPD            3           21         821        3282  \n",
       "4       BPD            1            4           1           9  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3fbca",
   "metadata": {
    "id": "c6b3fbca"
   },
   "source": [
    "### Data Downsizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d7dc34a",
   "metadata": {
    "id": "5d7dc34a",
    "outputId": "1551d8ae-dec6-4743-98d9-340e7544a8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 668096 entries, 0 to 701786\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   title        668096 non-null  object\n",
      " 1   selftext     668096 non-null  object\n",
      " 2   created_utc  668096 non-null  int64 \n",
      " 3   over_18      668096 non-null  bool  \n",
      " 4   subreddit    668096 non-null  object\n",
      " 5   title_total  668096 non-null  int64 \n",
      " 6   title_chars  668096 non-null  int64 \n",
      " 7   text_total   668096 non-null  int64 \n",
      " 8   text_chars   668096 non-null  int64 \n",
      "dtypes: bool(1), int64(5), object(3)\n",
      "memory usage: 46.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c485fe6",
   "metadata": {
    "id": "0c485fe6"
   },
   "source": [
    "The number of data is 666,8096. It is very large and takes a lot of time to process. As we wish to spotlight the posts published during the duration of the COVID-19 pandemic, we will be limiting our data to only include posts from March 2020 onwards. A random sample of 10,000 posts will be taken from the dataset for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a921dba9",
   "metadata": {
    "id": "a921dba9"
   },
   "outputs": [],
   "source": [
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b0d2016",
   "metadata": {
    "id": "0b0d2016",
    "outputId": "b8bbde38-1b24-44e5-feda-f80ef8225209"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_total</th>\n",
       "      <th>title_chars</th>\n",
       "      <th>text_total</th>\n",
       "      <th>text_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Life is so pointless without others</td>\n",
       "      <td>Does anyone else think the most important part...</td>\n",
       "      <td>2022-04-19 08:29:20</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>74</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cold rage?</td>\n",
       "      <td>Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...</td>\n",
       "      <td>2022-04-19 08:24:20</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>517</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I donâ€™t know who I am</td>\n",
       "      <td>My [F20] bf [M20] told me today (after I said ...</td>\n",
       "      <td>2022-04-19 08:02:59</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>145</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HELP! Opinions! Advice!</td>\n",
       "      <td>Okay, Iâ€™m about to open up about many things I...</td>\n",
       "      <td>2022-04-19 07:30:30</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>821</td>\n",
       "      <td>3282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>2022-04-19 06:48:27</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0  Life is so pointless without others   \n",
       "1                           Cold rage?   \n",
       "2                I donâ€™t know who I am   \n",
       "3              HELP! Opinions! Advice!   \n",
       "4                                 help   \n",
       "\n",
       "                                            selftext         created_utc  \\\n",
       "0  Does anyone else think the most important part... 2022-04-19 08:29:20   \n",
       "1  Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect... 2022-04-19 08:24:20   \n",
       "2  My [F20] bf [M20] told me today (after I said ... 2022-04-19 08:02:59   \n",
       "3  Okay, Iâ€™m about to open up about many things I... 2022-04-19 07:30:30   \n",
       "4                                          [removed] 2022-04-19 06:48:27   \n",
       "\n",
       "   over_18 subreddit  title_total  title_chars  text_total  text_chars  \n",
       "0    False       BPD            6           30          74         310  \n",
       "1    False       BPD            2            9         517        2259  \n",
       "2    False       BPD            6           16         145         545  \n",
       "3    False       BPD            3           21         821        3282  \n",
       "4    False       BPD            1            4           1           9  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2e15043",
   "metadata": {
    "id": "f2e15043"
   },
   "outputs": [],
   "source": [
    "# Filter posts from March 2020 onwards\n",
    "filtered_df = df[df['created_utc'] >= '2020-03-01']\n",
    "\n",
    "# Take a random sample of 10,000 posts\n",
    "sampled_df = filtered_df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "816488c1",
   "metadata": {
    "id": "816488c1",
    "outputId": "081f9bfc-ceba-416c-f79f-77f73f97da82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_total</th>\n",
       "      <th>title_chars</th>\n",
       "      <th>text_total</th>\n",
       "      <th>text_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131450</th>\n",
       "      <td>Looking for hope (feeling fed up)</td>\n",
       "      <td>My diagnosis is fairly new and I havent starte...</td>\n",
       "      <td>2020-05-30 22:47:57</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>344</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691395</th>\n",
       "      <td>Get motivated with determination you can do an...</td>\n",
       "      <td>Like I just managed to cut with a safety razor</td>\n",
       "      <td>2020-05-17 15:31:50</td>\n",
       "      <td>False</td>\n",
       "      <td>mentalillness</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275676</th>\n",
       "      <td>memory flashes</td>\n",
       "      <td>so, I used to have a really good memory\\n\\n&amp;am...</td>\n",
       "      <td>2022-10-13 18:02:41</td>\n",
       "      <td>False</td>\n",
       "      <td>bipolar</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>91</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392360</th>\n",
       "      <td>I'll never get to live in the fantasy land for...</td>\n",
       "      <td>I won't ever get to turn my fantasies into rea...</td>\n",
       "      <td>2022-03-01 07:58:19</td>\n",
       "      <td>False</td>\n",
       "      <td>depression</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>72</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313915</th>\n",
       "      <td>It's my 27 birthday and I don't know wtf with ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>2022-02-26 21:42:56</td>\n",
       "      <td>False</td>\n",
       "      <td>depression</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "131450                  Looking for hope (feeling fed up)   \n",
       "691395  Get motivated with determination you can do an...   \n",
       "275676                                     memory flashes   \n",
       "392360  I'll never get to live in the fantasy land for...   \n",
       "313915  It's my 27 birthday and I don't know wtf with ...   \n",
       "\n",
       "                                                 selftext         created_utc  \\\n",
       "131450  My diagnosis is fairly new and I havent starte... 2020-05-30 22:47:57   \n",
       "691395     Like I just managed to cut with a safety razor 2020-05-17 15:31:50   \n",
       "275676  so, I used to have a really good memory\\n\\n&am... 2022-10-13 18:02:41   \n",
       "392360  I won't ever get to turn my fantasies into rea... 2022-03-01 07:58:19   \n",
       "313915                                          [removed] 2022-02-26 21:42:56   \n",
       "\n",
       "        over_18      subreddit  title_total  title_chars  text_total  \\\n",
       "131450    False            BPD            6           28         344   \n",
       "691395    False  mentalillness            8           45          10   \n",
       "275676    False        bipolar            2           13          91   \n",
       "392360    False     depression           10           41          72   \n",
       "313915    False     depression           13           50           1   \n",
       "\n",
       "        text_chars  \n",
       "131450        1414  \n",
       "691395          37  \n",
       "275676         424  \n",
       "392360         288  \n",
       "313915           9  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe946ad",
   "metadata": {
    "id": "9fe946ad",
    "outputId": "9bfe5136-ebab-4d31-d9f6-08e0d078c128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "depression       297\n",
       "Anxiety          275\n",
       "BPD              247\n",
       "mentalillness     77\n",
       "bipolar           72\n",
       "schizophrenia     32\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cff516",
   "metadata": {
    "id": "d7cff516"
   },
   "source": [
    "### Recategorizing 'subreddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5185fb52",
   "metadata": {
    "id": "5185fb52"
   },
   "outputs": [],
   "source": [
    "# def mental_disorders(ex):\n",
    "#     if ex == 'BPD':\n",
    "#         return 'BPD'\n",
    "#     elif ex == 'bipolar':\n",
    "#         return 'bipolar'\n",
    "#     elif ex == 'Anxiety':\n",
    "#         return 'anxiety'\n",
    "#     elif ex == 'schizophrenia':\n",
    "#         return 'schizophrenia'\n",
    "#     elif ex == 'depression':\n",
    "#         return 'depression'\n",
    "#     else:\n",
    "#         return 'others'\n",
    "\n",
    "def mental_disorders(ex):\n",
    "    if ex== 'schizophrenia':\n",
    "        return 'schizophrenia'\n",
    "    elif ex == 'Anxiety':\n",
    "        return 'Anxiety'\n",
    "    else:\n",
    "        return 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda2f5aa",
   "metadata": {
    "id": "dda2f5aa"
   },
   "outputs": [],
   "source": [
    "sampled_df['subreddit'] = sampled_df['subreddit'].apply(mental_disorders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac02ab71",
   "metadata": {
    "id": "ac02ab71",
    "outputId": "7db0956a-ba62-4fb6-9a16-5ef169e7687a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_total</th>\n",
       "      <th>title_chars</th>\n",
       "      <th>text_total</th>\n",
       "      <th>text_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131450</th>\n",
       "      <td>Looking for hope (feeling fed up)</td>\n",
       "      <td>My diagnosis is fairly new and I havent starte...</td>\n",
       "      <td>2020-05-30 22:47:57</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>344</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691395</th>\n",
       "      <td>Get motivated with determination you can do an...</td>\n",
       "      <td>Like I just managed to cut with a safety razor</td>\n",
       "      <td>2020-05-17 15:31:50</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275676</th>\n",
       "      <td>memory flashes</td>\n",
       "      <td>so, I used to have a really good memory\\n\\n&amp;am...</td>\n",
       "      <td>2022-10-13 18:02:41</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>91</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392360</th>\n",
       "      <td>I'll never get to live in the fantasy land for...</td>\n",
       "      <td>I won't ever get to turn my fantasies into rea...</td>\n",
       "      <td>2022-03-01 07:58:19</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>72</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313915</th>\n",
       "      <td>It's my 27 birthday and I don't know wtf with ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>2022-02-26 21:42:56</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538129</th>\n",
       "      <td>tips for managing the AAAA ?</td>\n",
       "      <td>I've been medically diagnosed with a general a...</td>\n",
       "      <td>2021-08-06 01:54:21</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>113</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293850</th>\n",
       "      <td>Breakup depression and self isolated without r...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>2022-08-06 09:23:21</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402501</th>\n",
       "      <td>I really canâ€™t get out of this</td>\n",
       "      <td>The last month my depression reach its lowest ...</td>\n",
       "      <td>2022-07-05 22:33:20</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>168</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566855</th>\n",
       "      <td>anxiety over such insignificant thingsâ€¦.</td>\n",
       "      <td>just had to reschedule a doctorâ€™s appointment ...</td>\n",
       "      <td>2021-08-18 22:51:22</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>192</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357103</th>\n",
       "      <td>My dog died and I have nothing left.</td>\n",
       "      <td>My marriage isn't doing great. Dog was healthy...</td>\n",
       "      <td>2022-08-25 20:40:17</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>196</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443776</th>\n",
       "      <td>Spiraling out of control</td>\n",
       "      <td>Do you ever get to where you feel fine one min...</td>\n",
       "      <td>2022-05-09 02:03:26</td>\n",
       "      <td>True</td>\n",
       "      <td>others</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>146</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87944</th>\n",
       "      <td>Pms exacerbating neediness for fp</td>\n",
       "      <td>I've been working hard with my therapist on co...</td>\n",
       "      <td>2021-01-18 00:09:14</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>90</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496598</th>\n",
       "      <td>EMDR?</td>\n",
       "      <td>Has anyone here used emdr therapy before? And ...</td>\n",
       "      <td>2022-02-27 04:59:27</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>Is anyone on amitriptyline?</td>\n",
       "      <td>Whatâ€™s your experience with amitriptyline? Iâ€™m...</td>\n",
       "      <td>2020-06-30 08:41:15</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>88</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671805</th>\n",
       "      <td>is this my rejection sensitivity or am i allow...</td>\n",
       "      <td>hi there! \\nso about two days ago i had what s...</td>\n",
       "      <td>2021-01-25 13:54:05</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>16</td>\n",
       "      <td>63</td>\n",
       "      <td>239</td>\n",
       "      <td>1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412371</th>\n",
       "      <td>My friends donâ€™t care</td>\n",
       "      <td>Iâ€™ve been best friends with my group for over ...</td>\n",
       "      <td>2022-10-23 20:53:35</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>438</td>\n",
       "      <td>1806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613187</th>\n",
       "      <td>Getting anxious about coming to gym</td>\n",
       "      <td>I used to train by myself. Now I'm coming to n...</td>\n",
       "      <td>2021-07-20 11:43:14</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>49</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379648</th>\n",
       "      <td>1st time on medication</td>\n",
       "      <td>Holy crap, I have been chronically depressed f...</td>\n",
       "      <td>2022-03-15 18:59:20</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>269</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140144</th>\n",
       "      <td>A mom learning a lot about BPD and DBT.</td>\n",
       "      <td>My 17yr old daughter is now beginning her jour...</td>\n",
       "      <td>2020-07-22 04:36:36</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>93</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66199</th>\n",
       "      <td>Just received my diagnosis. Now what?</td>\n",
       "      <td>My doctor just gave me a Bpd diagnosis. He is ...</td>\n",
       "      <td>2021-01-05 18:01:15</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>104</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "131450                  Looking for hope (feeling fed up)   \n",
       "691395  Get motivated with determination you can do an...   \n",
       "275676                                     memory flashes   \n",
       "392360  I'll never get to live in the fantasy land for...   \n",
       "313915  It's my 27 birthday and I don't know wtf with ...   \n",
       "538129                       tips for managing the AAAA ?   \n",
       "293850  Breakup depression and self isolated without r...   \n",
       "402501                     I really canâ€™t get out of this   \n",
       "566855           anxiety over such insignificant thingsâ€¦.   \n",
       "357103               My dog died and I have nothing left.   \n",
       "443776                           Spiraling out of control   \n",
       "87944                   Pms exacerbating neediness for fp   \n",
       "496598                                              EMDR?   \n",
       "115086                        Is anyone on amitriptyline?   \n",
       "671805  is this my rejection sensitivity or am i allow...   \n",
       "412371                              My friends donâ€™t care   \n",
       "613187                Getting anxious about coming to gym   \n",
       "379648                             1st time on medication   \n",
       "140144            A mom learning a lot about BPD and DBT.   \n",
       "66199               Just received my diagnosis. Now what?   \n",
       "\n",
       "                                                 selftext         created_utc  \\\n",
       "131450  My diagnosis is fairly new and I havent starte... 2020-05-30 22:47:57   \n",
       "691395     Like I just managed to cut with a safety razor 2020-05-17 15:31:50   \n",
       "275676  so, I used to have a really good memory\\n\\n&am... 2022-10-13 18:02:41   \n",
       "392360  I won't ever get to turn my fantasies into rea... 2022-03-01 07:58:19   \n",
       "313915                                          [removed] 2022-02-26 21:42:56   \n",
       "538129  I've been medically diagnosed with a general a... 2021-08-06 01:54:21   \n",
       "293850                                          [removed] 2022-08-06 09:23:21   \n",
       "402501  The last month my depression reach its lowest ... 2022-07-05 22:33:20   \n",
       "566855  just had to reschedule a doctorâ€™s appointment ... 2021-08-18 22:51:22   \n",
       "357103  My marriage isn't doing great. Dog was healthy... 2022-08-25 20:40:17   \n",
       "443776  Do you ever get to where you feel fine one min... 2022-05-09 02:03:26   \n",
       "87944   I've been working hard with my therapist on co... 2021-01-18 00:09:14   \n",
       "496598  Has anyone here used emdr therapy before? And ... 2022-02-27 04:59:27   \n",
       "115086  Whatâ€™s your experience with amitriptyline? Iâ€™m... 2020-06-30 08:41:15   \n",
       "671805  hi there! \\nso about two days ago i had what s... 2021-01-25 13:54:05   \n",
       "412371  Iâ€™ve been best friends with my group for over ... 2022-10-23 20:53:35   \n",
       "613187  I used to train by myself. Now I'm coming to n... 2021-07-20 11:43:14   \n",
       "379648  Holy crap, I have been chronically depressed f... 2022-03-15 18:59:20   \n",
       "140144  My 17yr old daughter is now beginning her jour... 2020-07-22 04:36:36   \n",
       "66199   My doctor just gave me a Bpd diagnosis. He is ... 2021-01-05 18:01:15   \n",
       "\n",
       "        over_18 subreddit  title_total  title_chars  text_total  text_chars  \n",
       "131450    False    others            6           28         344        1414  \n",
       "691395    False    others            8           45          10          37  \n",
       "275676    False    others            2           13          91         424  \n",
       "392360    False    others           10           41          72         288  \n",
       "313915    False    others           13           50           1           9  \n",
       "538129    False   Anxiety            6           23         113         506  \n",
       "293850    False    others            7           49           1           9  \n",
       "402501    False    others            7           24         168         698  \n",
       "566855    False   Anxiety            5           36         192         860  \n",
       "357103    False    others            8           29         196         757  \n",
       "443776     True    others            4           21         146         650  \n",
       "87944     False    others            5           29          90         381  \n",
       "496598    False   Anxiety            1            5          14          61  \n",
       "115086    False    others            4           24          88         435  \n",
       "671805    False    others           16           63         239        1036  \n",
       "412371    False    others            4           18         438        1806  \n",
       "613187    False   Anxiety            6           30          49         221  \n",
       "379648    False    others            4           19         269        1148  \n",
       "140144    False    others            9           31          93         413  \n",
       "66199     False    others            6           32         104         420  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcfcb147",
   "metadata": {
    "id": "bcfcb147"
   },
   "outputs": [],
   "source": [
    "# We will remove the rows under selftext with have '[removed]'\n",
    "\n",
    "sampled_df = sampled_df[sampled_df['selftext'] != '[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d85a060",
   "metadata": {
    "id": "3d85a060",
    "outputId": "13eb90bd-5528-4e4a-db12-1cddc3c332cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_total</th>\n",
       "      <th>title_chars</th>\n",
       "      <th>text_total</th>\n",
       "      <th>text_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131450</th>\n",
       "      <td>Looking for hope (feeling fed up)</td>\n",
       "      <td>My diagnosis is fairly new and I havent starte...</td>\n",
       "      <td>2020-05-30 22:47:57</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>344</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691395</th>\n",
       "      <td>Get motivated with determination you can do an...</td>\n",
       "      <td>Like I just managed to cut with a safety razor</td>\n",
       "      <td>2020-05-17 15:31:50</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275676</th>\n",
       "      <td>memory flashes</td>\n",
       "      <td>so, I used to have a really good memory\\n\\n&amp;am...</td>\n",
       "      <td>2022-10-13 18:02:41</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>91</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392360</th>\n",
       "      <td>I'll never get to live in the fantasy land for...</td>\n",
       "      <td>I won't ever get to turn my fantasies into rea...</td>\n",
       "      <td>2022-03-01 07:58:19</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>72</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538129</th>\n",
       "      <td>tips for managing the AAAA ?</td>\n",
       "      <td>I've been medically diagnosed with a general a...</td>\n",
       "      <td>2021-08-06 01:54:21</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>113</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402501</th>\n",
       "      <td>I really canâ€™t get out of this</td>\n",
       "      <td>The last month my depression reach its lowest ...</td>\n",
       "      <td>2022-07-05 22:33:20</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>168</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566855</th>\n",
       "      <td>anxiety over such insignificant thingsâ€¦.</td>\n",
       "      <td>just had to reschedule a doctorâ€™s appointment ...</td>\n",
       "      <td>2021-08-18 22:51:22</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>192</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357103</th>\n",
       "      <td>My dog died and I have nothing left.</td>\n",
       "      <td>My marriage isn't doing great. Dog was healthy...</td>\n",
       "      <td>2022-08-25 20:40:17</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>196</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443776</th>\n",
       "      <td>Spiraling out of control</td>\n",
       "      <td>Do you ever get to where you feel fine one min...</td>\n",
       "      <td>2022-05-09 02:03:26</td>\n",
       "      <td>True</td>\n",
       "      <td>others</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>146</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87944</th>\n",
       "      <td>Pms exacerbating neediness for fp</td>\n",
       "      <td>I've been working hard with my therapist on co...</td>\n",
       "      <td>2021-01-18 00:09:14</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>90</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496598</th>\n",
       "      <td>EMDR?</td>\n",
       "      <td>Has anyone here used emdr therapy before? And ...</td>\n",
       "      <td>2022-02-27 04:59:27</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>Is anyone on amitriptyline?</td>\n",
       "      <td>Whatâ€™s your experience with amitriptyline? Iâ€™m...</td>\n",
       "      <td>2020-06-30 08:41:15</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>88</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671805</th>\n",
       "      <td>is this my rejection sensitivity or am i allow...</td>\n",
       "      <td>hi there! \\nso about two days ago i had what s...</td>\n",
       "      <td>2021-01-25 13:54:05</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>16</td>\n",
       "      <td>63</td>\n",
       "      <td>239</td>\n",
       "      <td>1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412371</th>\n",
       "      <td>My friends donâ€™t care</td>\n",
       "      <td>Iâ€™ve been best friends with my group for over ...</td>\n",
       "      <td>2022-10-23 20:53:35</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>438</td>\n",
       "      <td>1806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613187</th>\n",
       "      <td>Getting anxious about coming to gym</td>\n",
       "      <td>I used to train by myself. Now I'm coming to n...</td>\n",
       "      <td>2021-07-20 11:43:14</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>49</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379648</th>\n",
       "      <td>1st time on medication</td>\n",
       "      <td>Holy crap, I have been chronically depressed f...</td>\n",
       "      <td>2022-03-15 18:59:20</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>269</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140144</th>\n",
       "      <td>A mom learning a lot about BPD and DBT.</td>\n",
       "      <td>My 17yr old daughter is now beginning her jour...</td>\n",
       "      <td>2020-07-22 04:36:36</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>93</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66199</th>\n",
       "      <td>Just received my diagnosis. Now what?</td>\n",
       "      <td>My doctor just gave me a Bpd diagnosis. He is ...</td>\n",
       "      <td>2021-01-05 18:01:15</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>104</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27126</th>\n",
       "      <td>I have a thing for younger guys</td>\n",
       "      <td>First off Iâ€™d like to state that im sorry for ...</td>\n",
       "      <td>2022-08-21 04:35:03</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>479</td>\n",
       "      <td>1874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341384</th>\n",
       "      <td>I feel like I have lost in the game of life. H...</td>\n",
       "      <td>No matter what I do I am not finding happiness...</td>\n",
       "      <td>2022-05-10 22:04:39</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>19</td>\n",
       "      <td>58</td>\n",
       "      <td>130</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "131450                  Looking for hope (feeling fed up)   \n",
       "691395  Get motivated with determination you can do an...   \n",
       "275676                                     memory flashes   \n",
       "392360  I'll never get to live in the fantasy land for...   \n",
       "538129                       tips for managing the AAAA ?   \n",
       "402501                     I really canâ€™t get out of this   \n",
       "566855           anxiety over such insignificant thingsâ€¦.   \n",
       "357103               My dog died and I have nothing left.   \n",
       "443776                           Spiraling out of control   \n",
       "87944                   Pms exacerbating neediness for fp   \n",
       "496598                                              EMDR?   \n",
       "115086                        Is anyone on amitriptyline?   \n",
       "671805  is this my rejection sensitivity or am i allow...   \n",
       "412371                              My friends donâ€™t care   \n",
       "613187                Getting anxious about coming to gym   \n",
       "379648                             1st time on medication   \n",
       "140144            A mom learning a lot about BPD and DBT.   \n",
       "66199               Just received my diagnosis. Now what?   \n",
       "27126                     I have a thing for younger guys   \n",
       "341384  I feel like I have lost in the game of life. H...   \n",
       "\n",
       "                                                 selftext         created_utc  \\\n",
       "131450  My diagnosis is fairly new and I havent starte... 2020-05-30 22:47:57   \n",
       "691395     Like I just managed to cut with a safety razor 2020-05-17 15:31:50   \n",
       "275676  so, I used to have a really good memory\\n\\n&am... 2022-10-13 18:02:41   \n",
       "392360  I won't ever get to turn my fantasies into rea... 2022-03-01 07:58:19   \n",
       "538129  I've been medically diagnosed with a general a... 2021-08-06 01:54:21   \n",
       "402501  The last month my depression reach its lowest ... 2022-07-05 22:33:20   \n",
       "566855  just had to reschedule a doctorâ€™s appointment ... 2021-08-18 22:51:22   \n",
       "357103  My marriage isn't doing great. Dog was healthy... 2022-08-25 20:40:17   \n",
       "443776  Do you ever get to where you feel fine one min... 2022-05-09 02:03:26   \n",
       "87944   I've been working hard with my therapist on co... 2021-01-18 00:09:14   \n",
       "496598  Has anyone here used emdr therapy before? And ... 2022-02-27 04:59:27   \n",
       "115086  Whatâ€™s your experience with amitriptyline? Iâ€™m... 2020-06-30 08:41:15   \n",
       "671805  hi there! \\nso about two days ago i had what s... 2021-01-25 13:54:05   \n",
       "412371  Iâ€™ve been best friends with my group for over ... 2022-10-23 20:53:35   \n",
       "613187  I used to train by myself. Now I'm coming to n... 2021-07-20 11:43:14   \n",
       "379648  Holy crap, I have been chronically depressed f... 2022-03-15 18:59:20   \n",
       "140144  My 17yr old daughter is now beginning her jour... 2020-07-22 04:36:36   \n",
       "66199   My doctor just gave me a Bpd diagnosis. He is ... 2021-01-05 18:01:15   \n",
       "27126   First off Iâ€™d like to state that im sorry for ... 2022-08-21 04:35:03   \n",
       "341384  No matter what I do I am not finding happiness... 2022-05-10 22:04:39   \n",
       "\n",
       "        over_18 subreddit  title_total  title_chars  text_total  text_chars  \n",
       "131450    False    others            6           28         344        1414  \n",
       "691395    False    others            8           45          10          37  \n",
       "275676    False    others            2           13          91         424  \n",
       "392360    False    others           10           41          72         288  \n",
       "538129    False   Anxiety            6           23         113         506  \n",
       "402501    False    others            7           24         168         698  \n",
       "566855    False   Anxiety            5           36         192         860  \n",
       "357103    False    others            8           29         196         757  \n",
       "443776     True    others            4           21         146         650  \n",
       "87944     False    others            5           29          90         381  \n",
       "496598    False   Anxiety            1            5          14          61  \n",
       "115086    False    others            4           24          88         435  \n",
       "671805    False    others           16           63         239        1036  \n",
       "412371    False    others            4           18         438        1806  \n",
       "613187    False   Anxiety            6           30          49         221  \n",
       "379648    False    others            4           19         269        1148  \n",
       "140144    False    others            9           31          93         413  \n",
       "66199     False    others            6           32         104         420  \n",
       "27126     False    others            7           25         479        1874  \n",
       "341384    False    others           19           58         130         501  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b982cf",
   "metadata": {},
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74b01bc8",
   "metadata": {
    "id": "74b01bc8",
    "outputId": "469c851a-1cac-4601-9842-f191606a98c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "string.punctuation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "602e0791",
   "metadata": {
    "id": "602e0791",
    "outputId": "7f7c6d44-7af2-451e-daa5-90360e3e39fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_total</th>\n",
       "      <th>title_chars</th>\n",
       "      <th>text_total</th>\n",
       "      <th>text_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131450</th>\n",
       "      <td>Looking for hope (feeling fed up)</td>\n",
       "      <td>My diagnosis is fairly new and I havent starte...</td>\n",
       "      <td>2020-05-30 22:47:57</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>344</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691395</th>\n",
       "      <td>Get motivated with determination you can do an...</td>\n",
       "      <td>Like I just managed to cut with a safety razor</td>\n",
       "      <td>2020-05-17 15:31:50</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275676</th>\n",
       "      <td>memory flashes</td>\n",
       "      <td>so, I used to have a really good memory\\n\\n&amp;am...</td>\n",
       "      <td>2022-10-13 18:02:41</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>91</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392360</th>\n",
       "      <td>I'll never get to live in the fantasy land for...</td>\n",
       "      <td>I won't ever get to turn my fantasies into rea...</td>\n",
       "      <td>2022-03-01 07:58:19</td>\n",
       "      <td>False</td>\n",
       "      <td>others</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>72</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538129</th>\n",
       "      <td>tips for managing the AAAA ?</td>\n",
       "      <td>I've been medically diagnosed with a general a...</td>\n",
       "      <td>2021-08-06 01:54:21</td>\n",
       "      <td>False</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>113</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "131450                  Looking for hope (feeling fed up)   \n",
       "691395  Get motivated with determination you can do an...   \n",
       "275676                                     memory flashes   \n",
       "392360  I'll never get to live in the fantasy land for...   \n",
       "538129                       tips for managing the AAAA ?   \n",
       "\n",
       "                                                 selftext         created_utc  \\\n",
       "131450  My diagnosis is fairly new and I havent starte... 2020-05-30 22:47:57   \n",
       "691395     Like I just managed to cut with a safety razor 2020-05-17 15:31:50   \n",
       "275676  so, I used to have a really good memory\\n\\n&am... 2022-10-13 18:02:41   \n",
       "392360  I won't ever get to turn my fantasies into rea... 2022-03-01 07:58:19   \n",
       "538129  I've been medically diagnosed with a general a... 2021-08-06 01:54:21   \n",
       "\n",
       "        over_18 subreddit  title_total  title_chars  text_total  text_chars  \n",
       "131450    False    others            6           28         344        1414  \n",
       "691395    False    others            8           45          10          37  \n",
       "275676    False    others            2           13          91         424  \n",
       "392360    False    others           10           41          72         288  \n",
       "538129    False   Anxiety            6           23         113         506  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a94830d",
   "metadata": {
    "id": "0a94830d",
    "outputId": "40185dca-8439-4498-eb32-611bcc95657c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>538129</th>\n",
       "      <td>tips for managing the AAAA ? I've been medical...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566855</th>\n",
       "      <td>anxiety over such insignificant thingsâ€¦. just ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496598</th>\n",
       "      <td>EMDR? Has anyone here used emdr therapy before...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613187</th>\n",
       "      <td>Getting anxious about coming to gym I used to ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633253</th>\n",
       "      <td>Nicotine Helps me think I found without nicoti...</td>\n",
       "      <td>schizophrenia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 all_text      subreddit\n",
       "538129  tips for managing the AAAA ? I've been medical...        Anxiety\n",
       "566855  anxiety over such insignificant thingsâ€¦. just ...        Anxiety\n",
       "496598  EMDR? Has anyone here used emdr therapy before...        Anxiety\n",
       "613187  Getting anxious about coming to gym I used to ...        Anxiety\n",
       "633253  Nicotine Helps me think I found without nicoti...  schizophrenia"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df['all_text'] = sampled_df['title'] + \" \" + sampled_df['selftext']\n",
    "\n",
    "df = sampled_df[['all_text', 'subreddit']]\n",
    "df = df[df['subreddit'] != 'others']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b468939",
   "metadata": {
    "id": "2b468939"
   },
   "outputs": [],
   "source": [
    "# Define the abbreviations dictionary\n",
    "abbr_dict = {\n",
    "    \"'cause\": \"because\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"i ve\": \"i have\",\n",
    "    \"imma\": \"i am going to\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"not've\": \"not have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"there're\": \"there are\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when're\": \"when are\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where're\": \"where are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Define the function to replace the abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    # Replace 'â€™' with '\\'\n",
    "    text = re.sub('â€™', '\\'', text)\n",
    "\n",
    "    # Remove any word that starts with 'm' or 'f' followed by digits\n",
    "    text = re.sub(r'\\b[mf](\\d+)\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove any digit that is followed by 'm' or 'f'\n",
    "    text = re.sub(r'\\b(\\d+)[mf]\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace abbreviations with their full form\n",
    "    for word in text.split():\n",
    "        if word.lower() in abbr_dict:\n",
    "            text = re.sub(r'\\b{}\\b'.format(word), abbr_dict[word.lower()], text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Define the function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    emoji = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002500-\\U00002BEF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoji, '', text)\n",
    "\n",
    "def remove_html(data):\n",
    "    html_tag=re.compile(r'<.*?>')\n",
    "    data=html_tag.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    text = re.sub(r'[^\\w\\s\\']',' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7709a964",
   "metadata": {
    "id": "7709a964",
    "outputId": "65db3469-16d6-4c6f-a8fa-2541db6f218b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>538129</th>\n",
       "      <td>tips for managing the AAAA ? I've been medical...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[tips, managing, aaaa, medically, diagnosed, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566855</th>\n",
       "      <td>anxiety over such insignificant thingsâ€¦. just ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anxiety, insignificant, things, reschedule, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496598</th>\n",
       "      <td>EMDR? Has anyone here used emdr therapy before...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[emdr, anyone, used, emdr, therapy, effective]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613187</th>\n",
       "      <td>Getting anxious about coming to gym I used to ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[getting, anxious, coming, gym, used, train, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633253</th>\n",
       "      <td>Nicotine Helps me think I found without nicoti...</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>[nicotine, helps, think, found, without, nicot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 all_text      subreddit  \\\n",
       "538129  tips for managing the AAAA ? I've been medical...        Anxiety   \n",
       "566855  anxiety over such insignificant thingsâ€¦. just ...        Anxiety   \n",
       "496598  EMDR? Has anyone here used emdr therapy before...        Anxiety   \n",
       "613187  Getting anxious about coming to gym I used to ...        Anxiety   \n",
       "633253  Nicotine Helps me think I found without nicoti...  schizophrenia   \n",
       "\n",
       "                                                   tokens  \n",
       "538129  [tips, managing, aaaa, medically, diagnosed, g...  \n",
       "566855  [anxiety, insignificant, things, reschedule, d...  \n",
       "496598     [emdr, anyone, used, emdr, therapy, effective]  \n",
       "613187  [getting, anxious, coming, gym, used, train, c...  \n",
       "633253  [nicotine, helps, think, found, without, nicot...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning and tokenization\n",
    "def tokenization(text):\n",
    "    set_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text = replace_abbreviations(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_whitespaces(text)\n",
    "    text = remove_digits(text)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return [w for w in tokens if w not in set_stop_words]\n",
    "\n",
    "df['tokens']= df['all_text'].apply(lambda x: tokenization(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91a74b93",
   "metadata": {
    "id": "91a74b93",
    "outputId": "6b2b989c-885a-4a24-c5fb-095923422c29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>538129</th>\n",
       "      <td>tips for managing the AAAA ? I've been medical...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[tips, managing, aaaa, medically, diagnosed, g...</td>\n",
       "      <td>[tip, manage, aaaa, medically, diagnose, gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566855</th>\n",
       "      <td>anxiety over such insignificant thingsâ€¦. just ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anxiety, insignificant, things, reschedule, d...</td>\n",
       "      <td>[anxiety, insignificant, thing, reschedule, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496598</th>\n",
       "      <td>EMDR? Has anyone here used emdr therapy before...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[emdr, anyone, used, emdr, therapy, effective]</td>\n",
       "      <td>[emdr, anyone, use, emdr, therapy, effective]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613187</th>\n",
       "      <td>Getting anxious about coming to gym I used to ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[getting, anxious, coming, gym, used, train, c...</td>\n",
       "      <td>[get, anxious, come, gym, use, train, come, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633253</th>\n",
       "      <td>Nicotine Helps me think I found without nicoti...</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>[nicotine, helps, think, found, without, nicot...</td>\n",
       "      <td>[nicotine, help, think, find, without, nicotin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525248</th>\n",
       "      <td>Weird feeling the past couple weeks Hello, Iâ€™m...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[weird, feeling, past, couple, weeks, hello, f...</td>\n",
       "      <td>[weird, feel, past, couple, week, hello, f, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612722</th>\n",
       "      <td>Does anyone else spiral when listening to anxi...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anyone, else, spiral, listening, anxiety, sto...</td>\n",
       "      <td>[anyone, else, spiral, listen, anxiety, story,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526416</th>\n",
       "      <td>ðŸ˜”ðŸ˜”ðŸ˜” Hi anxious nowâ€¦ about possible broken glas...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[hi, anxious, possible, broken, glass, eye, th...</td>\n",
       "      <td>[hi, anxious, possible, break, glass, eye, tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519916</th>\n",
       "      <td>MRI tomorrow and I'm really nervous My neurolo...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[mri, tomorrow, really, nervous, neurologist, ...</td>\n",
       "      <td>[mri, tomorrow, really, nervous, neurologist, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491615</th>\n",
       "      <td>How can I deal with anxiety during exams? I ha...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[deal, anxiety, exams, diagnosed, generalized,...</td>\n",
       "      <td>[deal, anxiety, exam, diagnose, generalize, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475423</th>\n",
       "      <td>I debated in front of a large audience.. I had...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[debated, front, large, audience, first, debat...</td>\n",
       "      <td>[debate, front, large, audience, first, debate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557930</th>\n",
       "      <td>Any tips &amp;amp; strategies on how to manage bra...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[tips, amp, strategies, manage, brain, fog, he...</td>\n",
       "      <td>[tip, amp, strategy, manage, brain, fog, hello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537061</th>\n",
       "      <td>Why Is My Anxiety So Weird? (First Post Here) ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anxiety, weird, first, post, got, diagnosed, ...</td>\n",
       "      <td>[anxiety, weird, first, post, get, diagnose, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631654</th>\n",
       "      <td>Is it possible Is it possible that i have insi...</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>[possible, possible, insight, hallucinations, ...</td>\n",
       "      <td>[possible, possible, insight, hallucination, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558565</th>\n",
       "      <td>Is this anxiety? Multiple hours ago I was so e...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anxiety, multiple, hours, ago, extremely, diz...</td>\n",
       "      <td>[anxiety, multiple, hour, ago, extremely, dizz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475305</th>\n",
       "      <td>Panic Attack when i receive text from ex? I ge...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[panic, attack, receive, text, ex, get, panic,...</td>\n",
       "      <td>[panic, attack, receive, text, ex, get, panic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574395</th>\n",
       "      <td>Scared of telling my House Mate I want to get ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[scared, telling, house, mate, want, get, plac...</td>\n",
       "      <td>[scar, tell, house, mate, want, get, place, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574217</th>\n",
       "      <td>I Love My Sport But It Gives Me Anxiety I love...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[love, sport, gives, anxiety, love, sport, pla...</td>\n",
       "      <td>[love, sport, give, anxiety, love, sport, play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576061</th>\n",
       "      <td>Is my mom okay Do people always have signs of ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[mom, okay, people, always, signs, ms, worried...</td>\n",
       "      <td>[mom, okay, people, always, sign, m, worry, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638241</th>\n",
       "      <td>I saw a Psychiatrist for the first time yester...</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>[saw, psychiatrist, first, time, yesterday, fi...</td>\n",
       "      <td>[saw, psychiatrist, first, time, yesterday, fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 all_text      subreddit  \\\n",
       "538129  tips for managing the AAAA ? I've been medical...        Anxiety   \n",
       "566855  anxiety over such insignificant thingsâ€¦. just ...        Anxiety   \n",
       "496598  EMDR? Has anyone here used emdr therapy before...        Anxiety   \n",
       "613187  Getting anxious about coming to gym I used to ...        Anxiety   \n",
       "633253  Nicotine Helps me think I found without nicoti...  schizophrenia   \n",
       "525248  Weird feeling the past couple weeks Hello, Iâ€™m...        Anxiety   \n",
       "612722  Does anyone else spiral when listening to anxi...        Anxiety   \n",
       "526416  ðŸ˜”ðŸ˜”ðŸ˜” Hi anxious nowâ€¦ about possible broken glas...        Anxiety   \n",
       "519916  MRI tomorrow and I'm really nervous My neurolo...        Anxiety   \n",
       "491615  How can I deal with anxiety during exams? I ha...        Anxiety   \n",
       "475423  I debated in front of a large audience.. I had...        Anxiety   \n",
       "557930  Any tips &amp; strategies on how to manage bra...        Anxiety   \n",
       "537061  Why Is My Anxiety So Weird? (First Post Here) ...        Anxiety   \n",
       "631654  Is it possible Is it possible that i have insi...  schizophrenia   \n",
       "558565  Is this anxiety? Multiple hours ago I was so e...        Anxiety   \n",
       "475305  Panic Attack when i receive text from ex? I ge...        Anxiety   \n",
       "574395  Scared of telling my House Mate I want to get ...        Anxiety   \n",
       "574217  I Love My Sport But It Gives Me Anxiety I love...        Anxiety   \n",
       "576061  Is my mom okay Do people always have signs of ...        Anxiety   \n",
       "638241  I saw a Psychiatrist for the first time yester...  schizophrenia   \n",
       "\n",
       "                                                   tokens  \\\n",
       "538129  [tips, managing, aaaa, medically, diagnosed, g...   \n",
       "566855  [anxiety, insignificant, things, reschedule, d...   \n",
       "496598     [emdr, anyone, used, emdr, therapy, effective]   \n",
       "613187  [getting, anxious, coming, gym, used, train, c...   \n",
       "633253  [nicotine, helps, think, found, without, nicot...   \n",
       "525248  [weird, feeling, past, couple, weeks, hello, f...   \n",
       "612722  [anyone, else, spiral, listening, anxiety, sto...   \n",
       "526416  [hi, anxious, possible, broken, glass, eye, th...   \n",
       "519916  [mri, tomorrow, really, nervous, neurologist, ...   \n",
       "491615  [deal, anxiety, exams, diagnosed, generalized,...   \n",
       "475423  [debated, front, large, audience, first, debat...   \n",
       "557930  [tips, amp, strategies, manage, brain, fog, he...   \n",
       "537061  [anxiety, weird, first, post, got, diagnosed, ...   \n",
       "631654  [possible, possible, insight, hallucinations, ...   \n",
       "558565  [anxiety, multiple, hours, ago, extremely, diz...   \n",
       "475305  [panic, attack, receive, text, ex, get, panic,...   \n",
       "574395  [scared, telling, house, mate, want, get, plac...   \n",
       "574217  [love, sport, gives, anxiety, love, sport, pla...   \n",
       "576061  [mom, okay, people, always, signs, ms, worried...   \n",
       "638241  [saw, psychiatrist, first, time, yesterday, fi...   \n",
       "\n",
       "                                        lemmatized_tokens  \n",
       "538129  [tip, manage, aaaa, medically, diagnose, gener...  \n",
       "566855  [anxiety, insignificant, thing, reschedule, do...  \n",
       "496598      [emdr, anyone, use, emdr, therapy, effective]  \n",
       "613187  [get, anxious, come, gym, use, train, come, ne...  \n",
       "633253  [nicotine, help, think, find, without, nicotin...  \n",
       "525248  [weird, feel, past, couple, week, hello, f, ho...  \n",
       "612722  [anyone, else, spiral, listen, anxiety, story,...  \n",
       "526416  [hi, anxious, possible, break, glass, eye, tho...  \n",
       "519916  [mri, tomorrow, really, nervous, neurologist, ...  \n",
       "491615  [deal, anxiety, exam, diagnose, generalize, an...  \n",
       "475423  [debate, front, large, audience, first, debate...  \n",
       "557930  [tip, amp, strategy, manage, brain, fog, hello...  \n",
       "537061  [anxiety, weird, first, post, get, diagnose, g...  \n",
       "631654  [possible, possible, insight, hallucination, w...  \n",
       "558565  [anxiety, multiple, hour, ago, extremely, dizz...  \n",
       "475305  [panic, attack, receive, text, ex, get, panic,...  \n",
       "574395  [scar, tell, house, mate, want, get, place, ye...  \n",
       "574217  [love, sport, give, anxiety, love, sport, play...  \n",
       "576061  [mom, okay, people, always, sign, m, worry, mo...  \n",
       "638241  [saw, psychiatrist, first, time, yesterday, fi...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(text):\n",
    "    lemm_text = [word_lemmatizer.lemmatize(word, pos=\"v\") for word in text]\n",
    "    lemm_text = [word_lemmatizer.lemmatize(word, pos=\"n\") for word in lemm_text]\n",
    "    lemm_text = [word_lemmatizer.lemmatize(word, pos=\"a\") for word in lemm_text]\n",
    "    lemm_text = [word_lemmatizer.lemmatize(word, pos=\"r\") for word in lemm_text]\n",
    "    lemm_text = [word_lemmatizer.lemmatize(word, pos=\"s\") for word in lemm_text]\n",
    "    return lemm_text\n",
    "\n",
    "df['lemmatized_tokens'] = df['tokens'].apply(lambda x:lemmatization(x))\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9ab1d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>padded_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>538129</th>\n",
       "      <td>tips for managing the AAAA ? I've been medical...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[tips, managing, aaaa, medically, diagnosed, g...</td>\n",
       "      <td>[tip, manage, aaaa, medically, diagnose, gener...</td>\n",
       "      <td>[173, 233, 1749, 1750, 220, 234, 2, 167, 1751,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566855</th>\n",
       "      <td>anxiety over such insignificant thingsâ€¦. just ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anxiety, insignificant, things, reschedule, d...</td>\n",
       "      <td>[anxiety, insignificant, thing, reschedule, do...</td>\n",
       "      <td>[2, 1758, 13, 602, 128, 209, 222, 1759, 696, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496598</th>\n",
       "      <td>EMDR? Has anyone here used emdr therapy before...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[emdr, anyone, used, emdr, therapy, effective]</td>\n",
       "      <td>[emdr, anyone, use, emdr, therapy, effective]</td>\n",
       "      <td>[1277, 22, 73, 1277, 84, 826, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613187</th>\n",
       "      <td>Getting anxious about coming to gym I used to ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[getting, anxious, coming, gym, used, train, c...</td>\n",
       "      <td>[get, anxious, come, gym, use, train, come, ne...</td>\n",
       "      <td>[3, 34, 40, 1002, 73, 603, 40, 116, 1002, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633253</th>\n",
       "      <td>Nicotine Helps me think I found without nicoti...</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>[nicotine, helps, think, found, without, nicot...</td>\n",
       "      <td>[nicotine, help, think, find, without, nicotin...</td>\n",
       "      <td>[827, 15, 6, 54, 117, 827, 109, 168, 1004, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525248</th>\n",
       "      <td>Weird feeling the past couple weeks Hello, Iâ€™m...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[weird, feeling, past, couple, weeks, hello, f...</td>\n",
       "      <td>[weird, feel, past, couple, week, hello, f, ho...</td>\n",
       "      <td>[111, 1, 105, 235, 59, 293, 1279, 418, 73, 89,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612722</th>\n",
       "      <td>Does anyone else spiral when listening to anxi...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anyone, else, spiral, listening, anxiety, sto...</td>\n",
       "      <td>[anyone, else, spiral, listen, anxiety, story,...</td>\n",
       "      <td>[22, 52, 385, 311, 2, 194, 16, 1771, 421, 89, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526416</th>\n",
       "      <td>ðŸ˜”ðŸ˜”ðŸ˜” Hi anxious nowâ€¦ about possible broken glas...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[hi, anxious, possible, broken, glass, eye, th...</td>\n",
       "      <td>[hi, anxious, possible, break, glass, eye, tho...</td>\n",
       "      <td>[239, 34, 357, 188, 1015, 276, 839, 358, 14, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519916</th>\n",
       "      <td>MRI tomorrow and I'm really nervous My neurolo...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[mri, tomorrow, really, nervous, neurologist, ...</td>\n",
       "      <td>[mri, tomorrow, really, nervous, neurologist, ...</td>\n",
       "      <td>[840, 315, 12, 261, 841, 9, 840, 222, 116, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491615</th>\n",
       "      <td>How can I deal with anxiety during exams? I ha...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[deal, anxiety, exams, diagnosed, generalized,...</td>\n",
       "      <td>[deal, anxiety, exam, diagnose, generalize, an...</td>\n",
       "      <td>[92, 2, 472, 220, 1291, 2, 167, 27, 177, 1292,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475423</th>\n",
       "      <td>I debated in front of a large audience.. I had...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[debated, front, large, audience, first, debat...</td>\n",
       "      <td>[debate, front, large, audience, first, debate...</td>\n",
       "      <td>[1293, 713, 610, 1294, 43, 1293, 426, 610, 714...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557930</th>\n",
       "      <td>Any tips &amp;amp; strategies on how to manage bra...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[tips, amp, strategies, manage, brain, fog, he...</td>\n",
       "      <td>[tip, amp, strategy, manage, brain, fog, hello...</td>\n",
       "      <td>[173, 121, 846, 233, 129, 530, 293, 1023, 1792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537061</th>\n",
       "      <td>Why Is My Anxiety So Weird? (First Post Here) ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anxiety, weird, first, post, got, diagnosed, ...</td>\n",
       "      <td>[anxiety, weird, first, post, get, diagnose, g...</td>\n",
       "      <td>[2, 111, 43, 131, 3, 220, 475, 1028, 111, 4, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631654</th>\n",
       "      <td>Is it possible Is it possible that i have insi...</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>[possible, possible, insight, hallucinations, ...</td>\n",
       "      <td>[possible, possible, insight, hallucination, w...</td>\n",
       "      <td>[357, 357, 617, 537, 117, 617, 12, 617, 618, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558565</th>\n",
       "      <td>Is this anxiety? Multiple hours ago I was so e...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[anxiety, multiple, hours, ago, extremely, diz...</td>\n",
       "      <td>[anxiety, multiple, hour, ago, extremely, dizz...</td>\n",
       "      <td>[2, 477, 103, 85, 241, 1008, 96, 4, 1805, 1033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475305</th>\n",
       "      <td>Panic Attack when i receive text from ex? I ge...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[panic, attack, receive, text, ex, get, panic,...</td>\n",
       "      <td>[panic, attack, receive, text, ex, get, panic,...</td>\n",
       "      <td>[39, 35, 481, 400, 391, 3, 39, 35, 481, 400, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574395</th>\n",
       "      <td>Scared of telling my House Mate I want to get ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[scared, telling, house, mate, want, get, plac...</td>\n",
       "      <td>[scar, tell, house, mate, want, get, place, ye...</td>\n",
       "      <td>[46, 69, 118, 1034, 9, 3, 126, 27, 177, 425, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574217</th>\n",
       "      <td>I Love My Sport But It Gives Me Anxiety I love...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[love, sport, gives, anxiety, love, sport, pla...</td>\n",
       "      <td>[love, sport, give, anxiety, love, sport, play...</td>\n",
       "      <td>[196, 437, 72, 2, 196, 437, 338, 53, 1316, 53,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576061</th>\n",
       "      <td>Is my mom okay Do people always have signs of ...</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>[mom, okay, people, always, signs, ms, worried...</td>\n",
       "      <td>[mom, okay, people, always, sign, m, worry, mo...</td>\n",
       "      <td>[236, 438, 21, 44, 546, 1053, 41, 236, 91, 840...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638241</th>\n",
       "      <td>I saw a Psychiatrist for the first time yester...</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>[saw, psychiatrist, first, time, yesterday, fi...</td>\n",
       "      <td>[saw, psychiatrist, first, time, yesterday, fi...</td>\n",
       "      <td>[407, 487, 43, 8, 426, 203, 440, 426, 242, 101...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 all_text      subreddit  \\\n",
       "538129  tips for managing the AAAA ? I've been medical...        Anxiety   \n",
       "566855  anxiety over such insignificant thingsâ€¦. just ...        Anxiety   \n",
       "496598  EMDR? Has anyone here used emdr therapy before...        Anxiety   \n",
       "613187  Getting anxious about coming to gym I used to ...        Anxiety   \n",
       "633253  Nicotine Helps me think I found without nicoti...  schizophrenia   \n",
       "525248  Weird feeling the past couple weeks Hello, Iâ€™m...        Anxiety   \n",
       "612722  Does anyone else spiral when listening to anxi...        Anxiety   \n",
       "526416  ðŸ˜”ðŸ˜”ðŸ˜” Hi anxious nowâ€¦ about possible broken glas...        Anxiety   \n",
       "519916  MRI tomorrow and I'm really nervous My neurolo...        Anxiety   \n",
       "491615  How can I deal with anxiety during exams? I ha...        Anxiety   \n",
       "475423  I debated in front of a large audience.. I had...        Anxiety   \n",
       "557930  Any tips &amp; strategies on how to manage bra...        Anxiety   \n",
       "537061  Why Is My Anxiety So Weird? (First Post Here) ...        Anxiety   \n",
       "631654  Is it possible Is it possible that i have insi...  schizophrenia   \n",
       "558565  Is this anxiety? Multiple hours ago I was so e...        Anxiety   \n",
       "475305  Panic Attack when i receive text from ex? I ge...        Anxiety   \n",
       "574395  Scared of telling my House Mate I want to get ...        Anxiety   \n",
       "574217  I Love My Sport But It Gives Me Anxiety I love...        Anxiety   \n",
       "576061  Is my mom okay Do people always have signs of ...        Anxiety   \n",
       "638241  I saw a Psychiatrist for the first time yester...  schizophrenia   \n",
       "\n",
       "                                                   tokens  \\\n",
       "538129  [tips, managing, aaaa, medically, diagnosed, g...   \n",
       "566855  [anxiety, insignificant, things, reschedule, d...   \n",
       "496598     [emdr, anyone, used, emdr, therapy, effective]   \n",
       "613187  [getting, anxious, coming, gym, used, train, c...   \n",
       "633253  [nicotine, helps, think, found, without, nicot...   \n",
       "525248  [weird, feeling, past, couple, weeks, hello, f...   \n",
       "612722  [anyone, else, spiral, listening, anxiety, sto...   \n",
       "526416  [hi, anxious, possible, broken, glass, eye, th...   \n",
       "519916  [mri, tomorrow, really, nervous, neurologist, ...   \n",
       "491615  [deal, anxiety, exams, diagnosed, generalized,...   \n",
       "475423  [debated, front, large, audience, first, debat...   \n",
       "557930  [tips, amp, strategies, manage, brain, fog, he...   \n",
       "537061  [anxiety, weird, first, post, got, diagnosed, ...   \n",
       "631654  [possible, possible, insight, hallucinations, ...   \n",
       "558565  [anxiety, multiple, hours, ago, extremely, diz...   \n",
       "475305  [panic, attack, receive, text, ex, get, panic,...   \n",
       "574395  [scared, telling, house, mate, want, get, plac...   \n",
       "574217  [love, sport, gives, anxiety, love, sport, pla...   \n",
       "576061  [mom, okay, people, always, signs, ms, worried...   \n",
       "638241  [saw, psychiatrist, first, time, yesterday, fi...   \n",
       "\n",
       "                                        lemmatized_tokens  \\\n",
       "538129  [tip, manage, aaaa, medically, diagnose, gener...   \n",
       "566855  [anxiety, insignificant, thing, reschedule, do...   \n",
       "496598      [emdr, anyone, use, emdr, therapy, effective]   \n",
       "613187  [get, anxious, come, gym, use, train, come, ne...   \n",
       "633253  [nicotine, help, think, find, without, nicotin...   \n",
       "525248  [weird, feel, past, couple, week, hello, f, ho...   \n",
       "612722  [anyone, else, spiral, listen, anxiety, story,...   \n",
       "526416  [hi, anxious, possible, break, glass, eye, tho...   \n",
       "519916  [mri, tomorrow, really, nervous, neurologist, ...   \n",
       "491615  [deal, anxiety, exam, diagnose, generalize, an...   \n",
       "475423  [debate, front, large, audience, first, debate...   \n",
       "557930  [tip, amp, strategy, manage, brain, fog, hello...   \n",
       "537061  [anxiety, weird, first, post, get, diagnose, g...   \n",
       "631654  [possible, possible, insight, hallucination, w...   \n",
       "558565  [anxiety, multiple, hour, ago, extremely, dizz...   \n",
       "475305  [panic, attack, receive, text, ex, get, panic,...   \n",
       "574395  [scar, tell, house, mate, want, get, place, ye...   \n",
       "574217  [love, sport, give, anxiety, love, sport, play...   \n",
       "576061  [mom, okay, people, always, sign, m, worry, mo...   \n",
       "638241  [saw, psychiatrist, first, time, yesterday, fi...   \n",
       "\n",
       "                                          padded_encoding  \n",
       "538129  [173, 233, 1749, 1750, 220, 234, 2, 167, 1751,...  \n",
       "566855  [2, 1758, 13, 602, 128, 209, 222, 1759, 696, 4...  \n",
       "496598  [1277, 22, 73, 1277, 84, 826, 0, 0, 0, 0, 0, 0...  \n",
       "613187  [3, 34, 40, 1002, 73, 603, 40, 116, 1002, 14, ...  \n",
       "633253  [827, 15, 6, 54, 117, 827, 109, 168, 1004, 18,...  \n",
       "525248  [111, 1, 105, 235, 59, 293, 1279, 418, 73, 89,...  \n",
       "612722  [22, 52, 385, 311, 2, 194, 16, 1771, 421, 89, ...  \n",
       "526416  [239, 34, 357, 188, 1015, 276, 839, 358, 14, 1...  \n",
       "519916  [840, 315, 12, 261, 841, 9, 840, 222, 116, 13,...  \n",
       "491615  [92, 2, 472, 220, 1291, 2, 167, 27, 177, 1292,...  \n",
       "475423  [1293, 713, 610, 1294, 43, 1293, 426, 610, 714...  \n",
       "557930  [173, 121, 846, 233, 129, 530, 293, 1023, 1792...  \n",
       "537061  [2, 111, 43, 131, 3, 220, 475, 1028, 111, 4, 2...  \n",
       "631654  [357, 357, 617, 537, 117, 617, 12, 617, 618, 3...  \n",
       "558565  [2, 477, 103, 85, 241, 1008, 96, 4, 1805, 1033...  \n",
       "475305  [39, 35, 481, 400, 391, 3, 39, 35, 481, 400, 3...  \n",
       "574395  [46, 69, 118, 1034, 9, 3, 126, 27, 177, 425, 2...  \n",
       "574217  [196, 437, 72, 2, 196, 437, 338, 53, 1316, 53,...  \n",
       "576061  [236, 438, 21, 44, 546, 1053, 41, 236, 91, 840...  \n",
       "638241  [407, 487, 43, 8, 426, 203, 440, 426, 242, 101...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "# Counting the unique number of tokens for num_words in text_encoding\n",
    "\n",
    "lemmatized_words = [word for word_list in df['lemmatized_tokens'] for word in word_list]\n",
    "unique_words = len(set(lemmatized_words))\n",
    "\n",
    "# Encoding and padding\n",
    "\n",
    "def text_encoding(lemmatized_texts, num_words):\n",
    "    vocabulary = defaultdict(int)\n",
    "    fdist = nltk.FreqDist()\n",
    "\n",
    "    all_lemmatized_words = [word for word_list in lemmatized_texts for word in word_list]\n",
    "    \n",
    "    for word in all_lemmatized_words:\n",
    "        fdist[word] += 1\n",
    "\n",
    "    common_words = fdist.most_common(num_words)\n",
    "\n",
    "    for idx, word in enumerate(common_words):\n",
    "        vocabulary[word[0]] = (idx + 1)\n",
    "\n",
    "    encoded_texts = []\n",
    "    texts4encoding = []\n",
    "\n",
    "    for tokens in lemmatized_texts:\n",
    "        temp_codes = []\n",
    "        temp_words = []\n",
    "\n",
    "        for word in tokens:\n",
    "            if word in vocabulary.keys():\n",
    "                temp_codes.append(vocabulary[word])\n",
    "                temp_words.append(word)\n",
    "\n",
    "        encoded_texts.append(temp_codes)\n",
    "        texts4encoding.append(temp_words)\n",
    "\n",
    "    vector_size = max(len(x) for x in encoded_texts)\n",
    "\n",
    "    return encoded_texts, texts4encoding, vector_size\n",
    "\n",
    "def codes_padding(X_encoded_texts):\n",
    "    pad_value = 0\n",
    "    padded_codes = []\n",
    "\n",
    "    codes_from_texts = copy.deepcopy(X_encoded_texts)\n",
    "    \n",
    "    # vector_size in text_encoding\n",
    "    max_length = max(len(encoded_text) for encoded_text in codes_from_texts)\n",
    "\n",
    "    for encoded_text in codes_from_texts:\n",
    "        while len(encoded_text) < max_length:\n",
    "            encoded_text.append(pad_value)\n",
    "        padded_codes.append(encoded_text)\n",
    "\n",
    "    return padded_codes\n",
    "\n",
    "df['padded_encoding'] = codes_padding(text_encoding(df['lemmatized_tokens'], unique_words)[0])\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af475cd",
   "metadata": {},
   "source": [
    "### Model 1: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87321180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cc15cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassificationCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        conved = nn.functional.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2)\n",
    "        return self.fc(conved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7408fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = unique_words\n",
    "embed_dim = 100\n",
    "num_classes = df['subreddit'].nunique()\n",
    "\n",
    "model = TextClassificationCNN(vocab_size, embed_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2866490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e0a42",
   "metadata": {},
   "source": [
    "### Training and Testing @ 10 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fea170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47da212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.2462 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 2: Loss: 0.3327 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 3: Loss: 0.2730 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 4: Loss: 0.3074 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 5: Loss: 0.3093 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 6: Loss: 0.2234 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 7: Loss: 0.2709 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 8: Loss: 0.2455 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 9: Loss: 0.3304 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 10: Loss: 0.2769 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lemmatized_tokens = df['lemmatized_tokens'].tolist()\n",
    "\n",
    "# Filter out non-list elements\n",
    "lemmatized_tokens = [tokens for tokens in lemmatized_tokens if isinstance(tokens, list)]\n",
    "\n",
    "# Create a vocabulary for your tokens and assign an index to each unique token\n",
    "vocab = {token: idx for idx, token in enumerate(set(token for tokens in lemmatized_tokens for token in tokens))}\n",
    "\n",
    "# Convert each token to its corresponding index\n",
    "X_data = [[vocab[token] for token in tokens] for tokens in lemmatized_tokens]\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "X_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(tokens) for tokens in X_data], batch_first=True)\n",
    "\n",
    "# Convert labels to tensor\n",
    "y_data = torch.tensor(df['subreddit'].astype('category').cat.codes.tolist(), dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Now, we can continue with the rest of your training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Ensure that labels have the correct dimensions (batch_size)\n",
    "        labels = labels.squeeze(dim=1) if len(labels.size()) > 1 else labels\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Calculate testing accuracy\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "\n",
    "    print('Epoch {}: Loss: {:.4f} | Training Accuracy: {:.4f} | Testing Accuracy: {:.4f}'.format(epoch + 1, loss.item(), train_accuracy, test_accuracy))\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423bb78",
   "metadata": {},
   "source": [
    "### Training and Testing @ 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "925f3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6589a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.1596 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 2: Loss: 0.3776 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 3: Loss: 0.1967 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 4: Loss: 0.1692 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 5: Loss: 0.2386 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 6: Loss: 0.2419 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 7: Loss: 0.3378 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 8: Loss: 0.2422 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 9: Loss: 0.2384 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 10: Loss: 0.2185 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 11: Loss: 0.2951 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 12: Loss: 0.2172 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 13: Loss: 0.2180 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 14: Loss: 0.2981 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 15: Loss: 0.2666 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 16: Loss: 0.3226 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 17: Loss: 0.3673 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 18: Loss: 0.3317 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 19: Loss: 0.3284 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 20: Loss: 0.1956 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 21: Loss: 0.2421 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 22: Loss: 0.2412 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 23: Loss: 0.2687 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 24: Loss: 0.2688 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 25: Loss: 0.3529 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 26: Loss: 0.2154 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 27: Loss: 0.1882 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 28: Loss: 0.2076 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 29: Loss: 0.2150 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 30: Loss: 0.1911 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 31: Loss: 0.2418 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 32: Loss: 0.1902 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 33: Loss: 0.2922 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 34: Loss: 0.2164 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 35: Loss: 0.3222 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 36: Loss: 0.2900 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 37: Loss: 0.1935 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 38: Loss: 0.3627 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 39: Loss: 0.2143 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 40: Loss: 0.2351 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 41: Loss: 0.3250 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 42: Loss: 0.2944 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 43: Loss: 0.3926 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 44: Loss: 0.2149 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 45: Loss: 0.2380 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 46: Loss: 0.3957 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 47: Loss: 0.2389 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 48: Loss: 0.2107 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 49: Loss: 0.2603 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 50: Loss: 0.1681 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens = df['lemmatized_tokens'].tolist()\n",
    "\n",
    "# Filter out non-list elements\n",
    "lemmatized_tokens = [tokens for tokens in lemmatized_tokens if isinstance(tokens, list)]\n",
    "\n",
    "# Create a vocabulary for your tokens and assign an index to each unique token\n",
    "vocab = {token: idx for idx, token in enumerate(set(token for tokens in lemmatized_tokens for token in tokens))}\n",
    "\n",
    "# Convert each token to its corresponding index\n",
    "X_data = [[vocab[token] for token in tokens] for tokens in lemmatized_tokens]\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "X_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(tokens) for tokens in X_data], batch_first=True)\n",
    "\n",
    "# Convert labels to tensor\n",
    "y_data = torch.tensor(df['subreddit'].astype('category').cat.codes.tolist(), dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Now, we can continue with the rest of your training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Ensure that labels have the correct dimensions (batch_size)\n",
    "        labels = labels.squeeze(dim=1) if len(labels.size()) > 1 else labels\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Calculate testing accuracy\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "\n",
    "    print('Epoch {}: Loss: {:.4f} | Training Accuracy: {:.4f} | Testing Accuracy: {:.4f}'.format(epoch + 1, loss.item(), train_accuracy, test_accuracy))\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed0621",
   "metadata": {},
   "source": [
    "### Training and Testing @ 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83149abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d06024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.2336 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 2: Loss: 0.3303 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 3: Loss: 0.2367 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 4: Loss: 0.2112 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 5: Loss: 0.3272 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 6: Loss: 0.2626 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 7: Loss: 0.2368 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 8: Loss: 0.2634 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 9: Loss: 0.2643 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 10: Loss: 0.2893 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 11: Loss: 0.2121 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 12: Loss: 0.2337 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 13: Loss: 0.2096 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 14: Loss: 0.2616 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 15: Loss: 0.2345 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 16: Loss: 0.2615 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 17: Loss: 0.1889 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 18: Loss: 0.2933 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 19: Loss: 0.3187 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 20: Loss: 0.2606 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 21: Loss: 0.2104 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 22: Loss: 0.2351 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 23: Loss: 0.2614 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 24: Loss: 0.2103 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 25: Loss: 0.2344 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 26: Loss: 0.2888 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 27: Loss: 0.1900 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 28: Loss: 0.2918 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 29: Loss: 0.2885 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 30: Loss: 0.2596 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 31: Loss: 0.2339 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 32: Loss: 0.2916 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 33: Loss: 0.2892 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 34: Loss: 0.2886 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 35: Loss: 0.3164 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 36: Loss: 0.3170 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 37: Loss: 0.2344 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 38: Loss: 0.3228 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 39: Loss: 0.2596 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 40: Loss: 0.2332 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 41: Loss: 0.2601 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 42: Loss: 0.2332 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 43: Loss: 0.2088 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 44: Loss: 0.2328 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 45: Loss: 0.2893 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 46: Loss: 0.2093 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 47: Loss: 0.2088 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 48: Loss: 0.2596 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 49: Loss: 0.1859 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 50: Loss: 0.1648 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 51: Loss: 0.3625 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 52: Loss: 0.2581 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 53: Loss: 0.1875 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 54: Loss: 0.1842 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 55: Loss: 0.2063 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 56: Loss: 0.1847 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 57: Loss: 0.2896 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 58: Loss: 0.2318 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 59: Loss: 0.2584 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 60: Loss: 0.2581 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 61: Loss: 0.1868 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 62: Loss: 0.3562 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 63: Loss: 0.2581 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 64: Loss: 0.2860 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 65: Loss: 0.2576 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 66: Loss: 0.3186 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 67: Loss: 0.1882 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 68: Loss: 0.1633 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 69: Loss: 0.2047 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 70: Loss: 0.2312 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 71: Loss: 0.2578 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 72: Loss: 0.2568 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 73: Loss: 0.2859 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 74: Loss: 0.2322 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 75: Loss: 0.2571 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 76: Loss: 0.2860 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 77: Loss: 0.2317 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 78: Loss: 0.2318 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 79: Loss: 0.2569 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 80: Loss: 0.2068 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 81: Loss: 0.2059 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 82: Loss: 0.2051 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 83: Loss: 0.2072 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 84: Loss: 0.2046 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 85: Loss: 0.2323 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 86: Loss: 0.2302 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 87: Loss: 0.1678 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 88: Loss: 0.4431 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 89: Loss: 0.1891 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 90: Loss: 0.2569 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 91: Loss: 0.2296 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 92: Loss: 0.2563 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 93: Loss: 0.3158 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 94: Loss: 0.2094 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 95: Loss: 0.1801 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 96: Loss: 0.2066 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 97: Loss: 0.2560 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 98: Loss: 0.2570 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 99: Loss: 0.2276 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Epoch 100: Loss: 0.1866 | Training Accuracy: 0.9333 | Testing Accuracy: 0.9474\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens = df['lemmatized_tokens'].tolist()\n",
    "\n",
    "# Filter out non-list elements\n",
    "lemmatized_tokens = [tokens for tokens in lemmatized_tokens if isinstance(tokens, list)]\n",
    "\n",
    "# Create a vocabulary for your tokens and assign an index to each unique token\n",
    "vocab = {token: idx for idx, token in enumerate(set(token for tokens in lemmatized_tokens for token in tokens))}\n",
    "\n",
    "# Convert each token to its corresponding index\n",
    "X_data = [[vocab[token] for token in tokens] for tokens in lemmatized_tokens]\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "X_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(tokens) for tokens in X_data], batch_first=True)\n",
    "\n",
    "# Convert labels to tensor\n",
    "y_data = torch.tensor(df['subreddit'].astype('category').cat.codes.tolist(), dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Now, we can continue with the rest of your training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Ensure that labels have the correct dimensions (batch_size)\n",
    "        labels = labels.squeeze(dim=1) if len(labels.size()) > 1 else labels\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Calculate testing accuracy\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "\n",
    "    print('Epoch {}: Loss: {:.4f} | Training Accuracy: {:.4f} | Testing Accuracy: {:.4f}'.format(epoch + 1, loss.item(), train_accuracy, test_accuracy))\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0428741",
   "metadata": {},
   "source": [
    "### CNN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "000ae297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import Precision\n",
    "from torchmetrics import Recall\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3095bbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9474\n",
      "Precision: 0.8975\n",
      "Recall: 0.9474\n",
      "F1 Score: 0.9218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:  # Assuming you want to evaluate on the test set\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate predictions and convert to numpy arrays\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        # Append to the lists\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# For precision, recall, and F1 score, set the `average` parameter to 'weighted'\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1 Score: {:.4f}\".format(f1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
